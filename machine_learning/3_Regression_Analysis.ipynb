{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Regressian Analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm that finds **a model that best describes data** and predicts future result values according to input values.\n",
    "\n",
    "- example\n",
    "    - **Input Data**   \n",
    "    $X$: Average Temparature, $Y$: Ice cream sales volume\n",
    "    \n",
    "    - **Assumption**   \n",
    "    $Y {\\approx} {\\beta}_0 + {\\beta}_1X$ -> Let's find **proper ${\\beta}_0$ and ${\\beta}_1$!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that a model describing data is in **a straight line**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using *OLS(Ordinary Least Squares)* to evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define **the sum of the squares of the difference between the actual and predicted values** as the Loss function.\n",
    "\n",
    "Loss function: $\\frac{1}{N}\\sum_{i = 1}^N \\left( Y_i - {\\beta}_0 - {\\beta}_1 X_i \\right)^2$\n",
    "\n",
    "The smaller the value of the Loss function, the better the model. So, let's find ways to minimalize the value of the Loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that the two values that make the value of the Loss function smallest are ${\\beta}^*_0$ and ${\\beta}^*_1$.\n",
    "\n",
    "**Gradient descent** is not a single calculation of beta0 and beta1, but a gradual calculation from the initial value.\n",
    "\n",
    "How Gradient descent works:\n",
    "1. Initialize ${\\beta}_0$ and ${\\beta}_1$ to random value\n",
    "2. Get the value of Loss function using current value of ${\\beta}_0$ and ${\\beta}_1$\n",
    "3. Calculate **Gradient value** that lets you know how you should change the value of ${\\beta}_0$ and ${\\beta}_1$ to make the value of loss function smaller\n",
    "4. Update ${\\beta}_0$ and ${\\beta}_1$ using the Gradient value.\n",
    "5. Repeat 2~4 until the value of loss function changes no more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value for train_X : \n",
      "[6.2546398  4.18978504 3.32191889 3.92228833 5.6910886  3.79415077\n",
      " 3.47870087 6.74700964 6.7906856  4.86824749]\n",
      "\n",
      "Actual value : \n",
      "0    5.644131\n",
      "1    3.758766\n",
      "2    3.872333\n",
      "3    4.409904\n",
      "4    6.438450\n",
      "5    4.028278\n",
      "6    2.261060\n",
      "7    7.157690\n",
      "8    6.290974\n",
      "9    5.196929\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# You can easily get the model that minimalizes the value of Loss function using scikit-learn library.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = [8.70153760, 3.90825773, 1.89362433, 3.28730045, 7.39333004, 2.98984649, 2.25757240, 9.84450732, 9.94589513, 5.48321616]\n",
    "Y = [5.64413093, 3.75876583, 3.87233310, 4.40990425, 6.43845020, 4.02827829, 2.26105955, 7.15768995, 6.29097441, 5.19692852]\n",
    "\n",
    "train_X = pd.DataFrame(X, columns=['X']) # Feature data must be of DataFrame type.\n",
    "train_Y = pd.Series(Y) # Label data must be of Series type.\n",
    "\n",
    "# Training Model\n",
    "lrmodel = LinearRegression() # Initialize Model\n",
    "lrmodel.fit(train_X, train_Y) # Learning is performed by inputting preprocessed data using fit(Feature, Label)\n",
    "\n",
    "# Visualizing the result of learning\n",
    "plt.scatter(X, Y) \n",
    "plt.plot([0, 10], [lrmodel.intercept_, 10 * lrmodel.coef_[0] + lrmodel.intercept_], c='r') \n",
    "plt.xlim(0, 10) \n",
    "plt.ylim(0, 10) \n",
    "plt.title('Training Result')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Predicting Label data from train_X\n",
    "pred_X = lrmodel.predict(train_X)\n",
    "print('Predicted value for train_X : \\n{}\\n'.format(pred_X))\n",
    "print('Actual value : \\n{}'.format(train_Y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The characteristics of simple regression analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It can only be applied when there is one input value.\n",
    "- Use it if you want to intuitively interpret the relationship between the two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to predict the result value with multiple inputs, use **multiple linear regression**.\n",
    "\n",
    "$Y {\\approx} {\\beta}_0 + {\\beta}_1X_1 + {\\beta}_2X_2 + {\\beta}_3X_3 + ... + {\\beta}_MX_M$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function**, like a simple linear regression, is defined as the sum of the squares of the difference between the input value and the actual value.\n",
    "\n",
    "$\\frac{1}{N}\\sum_{i = 1}^N \\left( Y^i - ({\\beta}_0 + {\\beta}_1 X^i_1 + {\\beta}_2 X^i_2 + ... + {\\beta}_M X^i_M)\\right)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "\n",
    "Let's use **gradient descent** to minimalize the value of Loss function as we did in simple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data :\n",
      "   Unnamed: 0     TV  Radio  Newspaper  Sales\n",
      "0           1  230.1   37.8       69.2   22.1\n",
      "1           2   44.5   39.3       45.1   10.4\n",
      "2           3   17.2   45.9       69.3    9.3\n",
      "3           4  151.5   41.3       58.5   18.5\n",
      "4           5  180.8   10.8       58.4   12.9 \n",
      "\n",
      "train_X : \n",
      "        TV  Radio  Newspaper\n",
      "79   116.0    7.7       23.1\n",
      "197  177.0    9.3        6.4\n",
      "38    43.1   26.7       35.1\n",
      "24    62.3   12.6       18.3\n",
      "122  224.0    2.4       15.6 \n",
      "\n",
      "train_Y : \n",
      "79     11.0\n",
      "197    12.8\n",
      "38     10.1\n",
      "24      9.7\n",
      "122    11.6\n",
      "Name: Sales, dtype: float64 \n",
      "\n",
      "test_X : \n",
      "        TV  Radio  Newspaper\n",
      "95   163.3   31.6       52.9\n",
      "15   195.4   47.7       52.9\n",
      "30   292.9   28.3       43.2\n",
      "158   11.7   36.9       45.2\n",
      "128  220.3   49.0        3.2 \n",
      "\n",
      "test_Y : \n",
      "95     16.9\n",
      "15     22.4\n",
      "30     21.4\n",
      "158     7.3\n",
      "128    24.7\n",
      "Name: Sales, dtype: float64 \n",
      "\n",
      "beta_0: 2.979067\n",
      "beta_1: 0.044730\n",
      "beta_2: 0.189195\n",
      "beta_3: 0.002761\n",
      "\n",
      "\n",
      "test_X : \n",
      "        TV  Radio  Newspaper\n",
      "95   163.3   31.6       52.9\n",
      "15   195.4   47.7       52.9\n",
      "30   292.9   28.3       43.2\n",
      "158   11.7   36.9       45.2\n",
      "128  220.3   49.0        3.2\n",
      "115   75.1   35.0       52.7\n",
      "69   216.8   43.9       27.2\n",
      "170   50.0   11.6       18.4\n",
      "174  222.4    3.4       13.1\n",
      "45   175.1   22.5       31.5\n",
      "66    31.5   24.6        2.2\n",
      "182   56.2    5.7       29.7\n",
      "165  234.5    3.4       84.8\n",
      "78     5.4   29.9        9.4\n",
      "186  139.5    2.1       26.6\n",
      "177  170.2    7.8       35.2\n",
      "56     7.3   28.1       41.4\n",
      "152  197.6   23.3       14.2\n",
      "82    75.3   20.3       32.5\n",
      "68   237.4   27.5       11.0\n",
      "124  229.5   32.3       74.2\n",
      "16    67.8   36.6      114.0\n",
      "148   38.0   40.3       11.9\n",
      "93   250.9   36.5       72.3\n",
      "65    69.0    9.3        0.9\n",
      "60    53.5    2.0       21.4\n",
      "84   213.5   43.0       33.8\n",
      "67   139.3   14.5       10.2\n",
      "125   87.2   11.8       25.9\n",
      "132    8.4   27.2        2.1\n",
      "9    199.8    2.6       21.2\n",
      "18    69.2   20.5       18.3\n",
      "55   198.9   49.4       60.0\n",
      "75    16.9   43.7       89.4\n",
      "150  280.7   13.9       37.0\n",
      "104  238.2   34.3        5.3\n",
      "135   48.3   47.0        8.5\n",
      "137  273.7   28.9       59.7\n",
      "164  117.2   14.7        5.4\n",
      "76    27.5    1.6       20.7 \n",
      "\n",
      "Predicted value for text_X : \n",
      "[16.4080242  20.88988209 21.55384318 10.60850256 22.11237326 13.10559172\n",
      " 21.05719192  7.46101034 13.60634581 15.15506967  9.04831992  6.65328312\n",
      " 14.34554487  8.90349333  9.68959028 12.16494386  8.73628397 16.26507258\n",
      " 10.27759582 18.83109103 19.56036653 13.25103464 12.33620695 21.30695132\n",
      "  7.82740305  5.80957448 20.75753231 11.98138077  9.18349576  8.5066991\n",
      " 12.46646769 10.00337695 21.3876709  12.24966368 18.26661538 20.13766267\n",
      " 14.05514005 20.85411186 11.0174441   4.56899622]\n",
      "\n",
      "df1 : \n",
      "   Radio  TV  Newspaper\n",
      "0      0   0          0\n",
      "1      1   0          0\n",
      "2      0   1          0\n",
      "3      0   0          1\n",
      "4      1   1          1\n",
      "Predicted value for df1 : \n",
      "[2.97906734 3.02379686 3.16826239 2.98182845 3.21575302]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"./Advertising.csv\")\n",
    "\n",
    "print('Original Data :')\n",
    "print(df.head(),'\\n')\n",
    "\n",
    "# Delete ['Unnamed: 0'] column which is not used as input value\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "\"\"\"\n",
    "1. The sales variable, which is label data, is stored in Y and the rest in X.\n",
    "\"\"\"\n",
    "X = df.drop(columns=['Sales'])\n",
    "Y = df['Sales']\n",
    "\n",
    "\"\"\"\n",
    "2. Split data into learning data and evaluation data.\n",
    "\"\"\"\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print preprocessed data\n",
    "print('train_X : ')\n",
    "print(train_X.head(),'\\n')\n",
    "print('train_Y : ')\n",
    "print(train_Y.head(),'\\n')\n",
    "\n",
    "print('test_X : ')\n",
    "print(test_X.head(),'\\n')\n",
    "print('test_Y : ')\n",
    "print(test_Y.head(), '\\n')\n",
    "\n",
    "\"\"\"\n",
    "3. Initialize multiple linear regression model and perfomr model learning.\n",
    "\"\"\"\n",
    "\n",
    "lrmodel = LinearRegression()\n",
    "lrmodel.fit(train_X, train_Y)\n",
    "\n",
    "\"\"\"\n",
    "4. Get model after learning\n",
    "\"\"\"\n",
    "beta_0 = lrmodel.intercept_ # y intercept (basic sales volume)\n",
    "beta_1 = lrmodel.coef_[0] # coefficient for first variable (Radio)\n",
    "beta_2 = lrmodel.coef_[1] # coefficient for second variable (TV)\n",
    "beta_3 = lrmodel.coef_[2] # coefficient for third variable (Newspaper)\n",
    "\n",
    "print(\"beta_0: %f\" % beta_0)\n",
    "print(\"beta_1: %f\" % beta_1)\n",
    "print(\"beta_2: %f\" % beta_2)\n",
    "print(\"beta_3: %f\" % beta_3)\n",
    "print(\"\\n\")\n",
    "\n",
    "print('test_X : ')\n",
    "print(test_X, \"\\n\")\n",
    "\n",
    "\"\"\"\n",
    "5. Predict for test_X.\n",
    "\"\"\"\n",
    "pred_X = lrmodel.predict(test_X)\n",
    "print('Predicted value for text_X : \\n{}\\n'.format(pred_X))\n",
    "\n",
    "# Define new data, df1.\n",
    "df1 = pd.DataFrame(np.array([[0, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 1]]), columns=['Radio', 'TV', 'Newspaper'])\n",
    "print('df1 : ')\n",
    "print(df1)\n",
    "\n",
    "\"\"\"\n",
    "6. Predict for df1.\n",
    "\"\"\"\n",
    "pred_df1 = lrmodel.predict(df1)\n",
    "print('Predicted value for df1 : \\n{}'.format(pred_df1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The characteristics of simple regression analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can see how each input value affects the result value.\n",
    "- If the **correlation** between multiple input values is high, there is a possibility of losing reliability in the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Regression Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RSS (Residual Sum of Squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The sum of the squares of the error between the actual value and the predicted value.\n",
    "- $RSS = \\sum_{i = 1}^N \\left( Y_i - {\\beta}_0 - {\\beta}_1 X_i \\right)^2$\n",
    "- Because the error is used as it is, it depends on the size of the input value.\n",
    "- It is impossible to compare with the absolute value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE (Mean Squared Error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RSS divided by the number of data.\n",
    "- The smaller this value, the higher the performance of the model can be evaluated.\n",
    "- $MSE = \\frac{1}{N}\\sum_{i = 1}^N \\left( Y_i - {\\beta}_0 - {\\beta}_1 X_i \\right)^2$\n",
    "- In the case of the regression model, MSE is used as **loss function**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAE (Mean Absolute Error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $MSE = \\frac{1}{N}\\sum_{i = 1}^N \\left|( Y_i - {\\beta}_0 - {\\beta}_1 X_i \\right)|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE vs MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MSE: Sensitive to outliers.\n",
    "- MAE: It is useful when predicting both high and low volatility indicators.\n",
    "- Because the average is used as it is, it depends on the size of the input value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE_train : 2.705129\n",
      "MAE_train : 1.198468\n",
      "MSE_test : 3.174097\n",
      "MAE_test : 1.460757\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# get predicted value of train_X\n",
    "pred_train = lrmodel.predict(train_X)\n",
    "\n",
    "\"\"\"\n",
    "1.calculate MSE, MAE of train_X\n",
    "\"\"\"\n",
    "MSE_train = mean_squared_error(train_Y, pred_train)\n",
    "MAE_train = mean_absolute_error(train_Y, pred_train)\n",
    "print('MSE_train : %f' % MSE_train)\n",
    "print('MAE_train : %f' % MAE_train)\n",
    "\n",
    "# get predicted value of test_X\n",
    "pred_test = lrmodel.predict(test_X)\n",
    "\n",
    "\"\"\"\n",
    "2. calculate MSE, MAE of test_X\n",
    "\"\"\"\n",
    "MSE_test = mean_squared_error(test_Y, pred_test)\n",
    "MAE_test = mean_absolute_error(test_Y, pred_test)\n",
    "print('MSE_test : %f' % MSE_test)\n",
    "print('MAE_test : %f' % MAE_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $R^2$ (coefficient of determination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $R^2 = 1 - \\frac{RSS}{TSS}$\n",
    "\n",
    "- $TSS = \\sum_{i = 1}^N \\left( y_i - \\bar y \\right)^2$, $\\bar y = \\frac{1}{N}\\sum_{i = 1}^N y_i $\n",
    "\n",
    "- $RSS = \\sum_{i = 1}^N \\left( y_i - {\\beta}_0 - {\\beta}_1 x_i \\right)^2$\n",
    "- $R^2$ has a value closer to 1 as there is no error.\n",
    "- When $R^2$ is 0, it means that the model is a linear model that outputs an average value of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2_train : 0.895701\n",
      "R2_test : 0.899438\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# get predicted value of train_X\n",
    "pred_train = lrmodel.predict(train_X)\n",
    "\n",
    "\"\"\"\n",
    "1.calculate R2 score of train_X\n",
    "\"\"\"\n",
    "R2_train = r2_score(train_Y, pred_train)\n",
    "print('R2_train : %f' % R2_train)\n",
    "\n",
    "# get predicted value of test_X\n",
    "pred_test = lrmodel.predict(test_X)\n",
    "\n",
    "\"\"\"\n",
    "2. calculate R2 score of test_X\n",
    "\"\"\"\n",
    "R2_test = r2_score(test_Y, pred_test)\n",
    "print('R2_test : %f' % R2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
