{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "## 2️⃣ Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec\n",
    "\n",
    "**word2vec** is one of various methods of **word embedding**.   \n",
    "**word2vec** learns word embedding vectors by using the problem of predicting what words appear in a given context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 140)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "doc = [[\"서울에\", \"살고\", \"있는\", \"나는\", \"강아지와\", \"고양이를\", \"좋아한다\"]]\n",
    "\n",
    "w2v_model = Word2Vec(min_count=1, window=2, vector_size=300)\n",
    "w2v_model.build_vocab(doc)\n",
    "w2v_model.train(doc, total_examples=w2v_model.corpus_count, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('서울에', 0.13418062031269073), ('나는', 0.050058916211128235), ('좋아한다', 0.033167969435453415), ('고양이를', 0.025743037462234497), ('살고', 0.01304111909121275), ('있는', -0.03428904712200165)]\n",
      "0.02574304\n"
     ]
    }
   ],
   "source": [
    "similar_word = w2v_model.wv.most_similar(\"강아지와\")\n",
    "print(similar_word)\n",
    "\n",
    "score = w2v_model.wv.similarity(\"강아지와\", \"고양이를\")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters of Word2Vec()\n",
    "- min_count: The model does not train words that appear less than min_count.\n",
    "- window: The model trains each words based on \"window\" words at the front and back of the word.\n",
    "- vector_size: defines the size of word embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('excited', 0.9061278700828552), ('thrilled', 0.9044565558433533), ('determined', 0.8859072327613831), ('stubborn', 0.8718532919883728), ('truthful', 0.8653934001922607), ('blessed', 0.8630205988883972), ('thankful', 0.8628314137458801), ('eager', 0.854218065738678), ('pleased', 0.8402495384216309), ('delighted', 0.8355127573013306)]\n",
      "[('scared', 0.9373880624771118), ('unhappy', 0.9362308382987976), ('hopeless', 0.93414705991745), ('lonely', 0.9294252395629883), ('angry', 0.9264203906059265), ('depressed', 0.9154171347618103), ('paranoid', 0.9112583994865417), ('worthless', 0.9094374179840088), ('nervous', 0.9087405204772949), ('bitter', 0.9044232964515686)]\n",
      "0.7675484\n",
      "0.92942524\n",
      "[-0.14070337  0.10991926 -0.082832    0.05044246 -0.16845034 -0.17876713\n",
      "  0.06207728  0.2871339  -0.0346562  -0.03207461  0.08975355 -0.14058274\n",
      " -0.0584864  -0.28872845 -0.06976887 -0.1127381   0.4116943  -0.07580606\n",
      " -0.08733585 -0.07632934 -0.00583419  0.04894549  0.09784348 -0.1811314\n",
      "  0.12294561 -0.0331579  -0.30580744  0.06227818 -0.23801525 -0.3317101\n",
      "  0.02080943  0.06682466  0.08747552 -0.3211472  -0.25141406  0.32541215\n",
      "  0.20398903 -0.21314293 -0.09968037  0.06669234 -0.13114965  0.11062892\n",
      "  0.08069657 -0.17739879  0.17851114  0.00965534  0.09093623  0.04799072\n",
      "  0.127168    0.12837616  0.2537321   0.0793937  -0.12264094  0.01097581\n",
      " -0.22410433 -0.00109726  0.07238921  0.03059815 -0.11696452 -0.12994361\n",
      " -0.12517804  0.11889207 -0.1711614   0.17811356 -0.04561207  0.04632137\n",
      " -0.18316743  0.44177318 -0.18570313 -0.14131571  0.05303061  0.14735566\n",
      "  0.1002725  -0.21529952  0.10990808  0.22874323 -0.0380578  -0.041073\n",
      " -0.01981093  0.30579668 -0.01579737 -0.21608344  0.02886483  0.4184688\n",
      " -0.11767654 -0.05432852  0.11528578  0.0773433  -0.06690871  0.19784276\n",
      "  0.21953659 -0.09812588  0.12988679  0.19303936  0.15691721  0.15108408\n",
      " -0.12597403 -0.02355799 -0.08644686  0.09502639 -0.03886806 -0.04310569\n",
      "  0.17503314 -0.03456855 -0.07677195 -0.42747593  0.10122613 -0.19008327\n",
      " -0.23684894 -0.28546777 -0.20257407 -0.1477885  -0.03138509  0.05823305\n",
      "  0.09511012  0.07291234 -0.18446583  0.05626968  0.11336523 -0.23039939\n",
      "  0.07052336  0.02476737  0.23341241  0.08885584  0.03492893  0.26342526\n",
      "  0.016494   -0.05185403 -0.1677635   0.11551891  0.11598776  0.22116257\n",
      " -0.01440112 -0.28197905 -0.10878818  0.3466291   0.05396545  0.00713861\n",
      " -0.27837804  0.0119263   0.16889283 -0.3277852  -0.22868758  0.00674314\n",
      "  0.03627333 -0.08901545 -0.20900582 -0.1139264   0.10913607 -0.07150061\n",
      "  0.25366676 -0.5866908  -0.02418886 -0.07569797 -0.22800961 -0.07470516\n",
      " -0.15712447 -0.2556972   0.04885122  0.14701824 -0.2360246  -0.06111543\n",
      " -0.31078604  0.1241899  -0.10769742  0.05855914  0.04501558 -0.08452375\n",
      " -0.24393992  0.52692586 -0.13997138  0.24447356  0.05001669  0.3712251\n",
      "  0.03683899  0.10163451 -0.13452514 -0.20708662  0.07274895 -0.08712693\n",
      " -0.05908895  0.16565336 -0.18521538 -0.21720588  0.03976181 -0.0445886\n",
      "  0.38584936  0.02429952 -0.11436057 -0.35678756  0.06746457 -0.01161471\n",
      " -0.41796768 -0.12478383  0.0693098  -0.16622616  0.04235152 -0.2742367\n",
      "  0.1611627   0.02054902 -0.15430881  0.09540475  0.01209121 -0.31138808\n",
      " -0.16148937 -0.15364246 -0.24324359  0.0988005  -0.1383733   0.04447366\n",
      " -0.08664148 -0.02558004 -0.00629082 -0.22566111  0.19337885 -0.2875632\n",
      "  0.06751181 -0.3863662  -0.34891522  0.0490298   0.25056702 -0.01691149\n",
      " -0.06058849 -0.10815214 -0.15156002  0.07751632  0.15947737 -0.0252179\n",
      " -0.08092684  0.04370841  0.2008713  -0.0551501  -0.31199658  0.06055518\n",
      " -0.10957547  0.05272757 -0.02133082  0.05375514 -0.16675647 -0.16731216\n",
      "  0.02016967  0.10812413 -0.09501543 -0.05790669  0.02863815 -0.17800632\n",
      " -0.02335519 -0.04144952  0.19484401  0.04162746  0.07817724 -0.16346805\n",
      "  0.21835083  0.19231737 -0.38263318 -0.17478442  0.20544003  0.20704363\n",
      " -0.33917317  0.10252815 -0.04211003  0.19734545  0.25152284 -0.36691913\n",
      " -0.54704154 -0.01520401 -0.00304671  0.12603539 -0.18570882  0.26694942\n",
      " -0.2365345   0.16223174  0.09965577 -0.02235252  0.07285836  0.41459206\n",
      "  0.18060857  0.17783159 -0.20814894 -0.0105041   0.22167946 -0.1996286\n",
      " -0.11926243 -0.10766893 -0.01831501  0.02339033 -0.41264614 -0.05256538\n",
      "  0.07242251  0.38276428 -0.08197444  0.06249371  0.00846639  0.04285334\n",
      "  0.21270756  0.24994817  0.05313817 -0.07089433  0.16649397 -0.0475872 ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def load_data(filepath):\n",
    "    data = pd.read_csv(filepath, delimiter=';', header=None, names=['sentence','emotion'])\n",
    "    data = data['sentence']\n",
    "\n",
    "    gensim_input = []\n",
    "    for text in data:\n",
    "        gensim_input.append(text.rstrip().split())\n",
    "    return gensim_input\n",
    "\n",
    "input_data = load_data(\"emotions_train.txt\")\n",
    "\n",
    "# Train word2vec model\n",
    "\n",
    "w2v_model = Word2Vec(window = 2, vector_size = 300)\n",
    "w2v_model.build_vocab(input_data)\n",
    "w2v_model.train(input_data, total_examples=w2v_model.corpus_count, epochs=10)\n",
    "\n",
    "# Check which word is similar with \"happy\".\n",
    "similar_happy = w2v_model.wv.most_similar(\"happy\")\n",
    "\n",
    "print(similar_happy)\n",
    "\n",
    "# Check which word is similar with \"sad\".\n",
    "similar_sad = w2v_model.wv.most_similar(\"sad\")\n",
    "\n",
    "print(similar_sad)\n",
    "\n",
    "# Check similarity between \"good\" and \"bad\".\n",
    "similar_good_bad = w2v_model.wv.similarity(\"good\", \"bad\")\n",
    "\n",
    "print(similar_good_bad)\n",
    "\n",
    "# Check similarity between \"sad\" and \"lonely\".\n",
    "similar_sad_lonely = w2v_model.wv.similarity(\"sad\", \"lonely\")\n",
    "\n",
    "print(similar_sad_lonely)\n",
    "\n",
    "# Check embedding vector of \"happy\".\n",
    "wv_happy = w2v_model.wv[\"happy\"]\n",
    "\n",
    "print(wv_happy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### out-of-vocabulary problem\n",
    "\n",
    "word2vec has a problem in that it is not possible to generate an embedding vector for a word that is not in the learning data. -> **fastText** can solve this problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fastText\n",
    "\n",
    "fastText splits each word into **letters** and trains the model similarly to word2vec so that they can create embedding vector for the word that is not in the trian data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "doc = [[\"서울에\", \"살고\", \"있는\", \"나는\", \"강아지와\", \"고양이를\", \"좋아한다\"]]\n",
    "\n",
    "ft_model = FastText(min_count=1, window=2, vector_size=300)\n",
    "ft_model.build_vocab(doc)\n",
    "ft_model.train(doc, total_examples=ft_model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_word = ft_model.wv.most_similar(\"엘리스는\")\n",
    "print(similar_word)\n",
    "# [('좋아한다', 0.03110547922551632),\n",
    "# ('살고', 0.015657681971788406),\n",
    "# ('강아지를', -0.09297232329845428),\n",
    "# ('서울에', -0.10255782306194305),\n",
    "# ('있는', -0.10588616132736206b)]\n",
    "\n",
    "new_vector = ft_model.wv[\"좋아한다고\"]\n",
    "print(new_vector)\n",
    "# array([-5.8544584e-04, -1.5485507e-03, -1.3994898e-03, -9.1309723e-04, ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
