{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "## 3️⃣ Emotion Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navie Bayes Classification\n",
    "\n",
    "$$P(Emotion|Text) = \\frac{P(Text|Emotion) * P(Emotion)}{P(Text)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use **Bayes Theorem** to predict the probability of occurrence of each emotions in text.\n",
    "\n",
    "#### Likelihood of text\n",
    "\n",
    "The **likelihood of Text** can be calculated by frequency of words.   \n",
    "$$P(Word | Emotion) = \\frac{\\text{(Frequency of Word in the emotion)}}{\\text{(Frequency of every word in the emotion)}}$$\n",
    "\n",
    "For example: \n",
    "$$P(fun | Happy) = \\frac{\\text{(Frequency of \"fun\" in sentences that expesses happiness)}}{\\text{(Frequency of every word in sentences that expresses happiness)}}$$\n",
    "\n",
    "#### Likelihood of Emotion\n",
    "\n",
    "The **likelihood of Emotion** is calculated as the ratio of sentences expressing the emotion within the entire data.\n",
    "$$P(Emotion)=\\frac{\\text{(Number of sentences that express the emotion)}}{\\text{(Number of sentences in entire data)}}$$\n",
    "\n",
    "After calculating $P(Emotion|Text)$ for every emotions, the model predicts the emotion with the greatest value among each probability as the emotion in the corresponding sentence.\n",
    "\n",
    "#### Likelihood of Sentence\n",
    "\n",
    "The **likelihood of Sentence** is calculated by multiplying likelihood of each words in the sentence and likelihood of the emotion. For detailed description, click [here](https://web.stanford.edu/~jurafsky/slp3/4.pdf).\n",
    "$$\\text{Probability that sentence expresses Emotion} = P(Word_1 | Emotion) * ... * P(Word_n | Emotion) * P(Emotion)$$\n",
    "\n",
    "#### Smoothing\n",
    "\n",
    "In the case of words that **do not exist in the learning data**, the likelihood  becomes zero. To prevent this, the frequency of words that do not exist in the learning data must be corrected by **smoothing** as below :\n",
    "\n",
    "$$P(Word | Emotion) = \\frac{\\text{(Frequency of Word in the emotion)} + 1}{\\text{(Frequency of every word in the emotion)} + 1}$$\n",
    "\n",
    "#### Calculating the probability using the log\n",
    "\n",
    "To calculate likelihood of a **sentence**, model multiplies likelihoods of several words, which makes the result **too small** for the computer to express the value any longer. So, the model uses **log** to express the likelihood as below :\n",
    "\n",
    "$$log(P(Sentence | Emotion)) = log(P(Word_1 | Emotion)) + ... + log(P(Word_n | Emotion)) + log(P(Emotion))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('surprise', -49.413280143234715)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# calculates the frequency at which each word appears in a sentence representing corresponding emotion,\n",
    "# and put that frequency into partial_freq.\n",
    "def cal_partial_freq(texts, emotion):\n",
    "    filtered_texts = texts[texts['emotion'] == emotion]\n",
    "    filtered_texts = filtered_texts['sentence']\n",
    "    partial_freq = dict()\n",
    "\n",
    "    for line in filtered_texts:\n",
    "        for word in line.rstrip().split():\n",
    "            if word not in partial_freq:\n",
    "                partial_freq[word] = 1\n",
    "            else :\n",
    "                partial_freq[word] += 1\n",
    "    \n",
    "    return partial_freq\n",
    "\n",
    "# calculates the sum of frequencies of every words in partial_freq\n",
    "def cal_total_freq(partial_freq):\n",
    "    total = 0\n",
    "    for word, freq in partial_freq.items() :\n",
    "        total += freq\n",
    "    return total\n",
    "\n",
    "# return log likelihood of coressponding emotion\n",
    "def cal_prior_prob(data, emotion):\n",
    "    filtered_texts = data[data['emotion'] == emotion]\n",
    "    return np.log(len(filtered_texts) / len(data))\n",
    "\n",
    "# Predict which emotion does corresponding sentence 'sent' expresses.\n",
    "def predict_emotion(sent, data):\n",
    "    emotions = ['anger', 'love', 'sadness', 'fear', 'joy', 'surprise']\n",
    "    predictions = []\n",
    "    train_txt = pd.read_csv(data, delimiter=';', header=None, names=['sentence', 'emotion'])\n",
    "\n",
    "    # Save each emotion's lieklihood of sent to predictions.\n",
    "    for emotion in emotions:\n",
    "        emotion_prob = cal_prior_prob(train_txt, emotion)\n",
    "        emotion_counter = cal_partial_freq(train_txt, emotion)\n",
    "        prob = 0\n",
    "        for word in sent.split() :\n",
    "            # Set smoothing to 10.\n",
    "            prob += np.log((emotion_counter[word] + 10) / (cal_total_freq(emotion_counter) + 10))\n",
    "        prob += emotion_prob\n",
    "        predictions.append((emotion, prob))\n",
    "        predictions.sort(key = lambda a: a[1])\n",
    "    return predictions[-1]\n",
    "\n",
    "test_sent = \"i really want to go and enjoy this party\"\n",
    "predicted = predict_emotion(test_sent, \"emotions_train.txt\")\n",
    "print(predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Naive Bayes classification using scikit-learn\n",
    "\n",
    "To implement Naive Bayes classification using scikit-learn, sentences in data for training must be transformed into **vectors** using **CountVectorizer** from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 2)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "doc = [\"i am very happy\", \"this product is really great\"]\n",
    "emotion = [\"happy\", \"excited\"]\n",
    "\n",
    "cv = CountVectorizer()\n",
    "csr_doc_matrix = cv.fit_transform(doc)\n",
    "\n",
    "print(csr_doc_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After transforming the data, we can train the model as below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(csr_doc_matrix, emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict test sentence, we must transform the sentence for test into vector too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['excited']\n"
     ]
    }
   ],
   "source": [
    "test_doc = [\"i am really great\"]\n",
    "\n",
    "transformed_test = cv.transform(test_doc)\n",
    "pred = clf.predict(transformed_test)\n",
    "\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
