{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "## 4️⃣ Korean Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristic of Korean\n",
    "\n",
    "- In general, Natural Language Processing models predict sentence's meaning and characteristics using **words** of the sentence.\n",
    "- However, the **criteria for Korean words are not clear**.\n",
    "- In Korean, words are composed of **a combination of a semantic part and a grammatical part.** (i.e. 먹- + -다, 먹- + -었다, 먹- + -는, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KoNLPy\n",
    "\n",
    "**KoNLPy** is a python library that **extract korean words** from a sentence using several korean '형태소' dictionaries. (한나눔, 꼬꼬마, Komoran, Mecab, Open Korean Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕', '우혁', '너', '이름', '뭐']\n",
      "[('안녕', 'NNG'), ('나', 'VV'), ('는', 'ETD'), ('우혁', 'UN'), ('이야', 'JX'), ('반갑', 'VA'), ('어', 'ECS'), ('.', 'SF'), ('너', 'NP'), ('의', 'JKG'), ('이름', 'NNG'), ('은', 'JX'), ('뭐', 'NP'), ('야', 'JX'), ('?', 'SF')]\n",
      "['안녕 나는 우혁이야 반가워. 너의 이름은 뭐야?']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "\n",
    "sent = \"안녕 나는 우혁이야 반가워. 너의 이름은 뭐야?\"\n",
    "\n",
    "kkma = Kkma()\n",
    "\n",
    "print(kkma.nouns(sent))\n",
    "print(kkma.pos(sent)) # returns each word and each word's 형태소 품사\n",
    "print(kkma.sentences(sent)) # Split sent to each sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕', '나', '우혁', '너', '이름', '뭐']\n",
      "[('안녕', 'Noun'), ('나', 'Noun'), ('는', 'Josa'), ('우혁', 'Noun'), ('이야', 'Josa'), ('반가워', 'Adjective'), ('.', 'Punctuation'), ('너', 'Noun'), ('의', 'Josa'), ('이름', 'Noun'), ('은', 'Josa'), ('뭐', 'Noun'), ('야', 'Josa'), ('?', 'Punctuation')]\n",
      "[('안녕', 'Noun'), ('나', 'Noun'), ('는', 'Josa'), ('우혁', 'Noun'), ('이야', 'Josa'), ('반갑다', 'Adjective'), ('.', 'Punctuation'), ('너', 'Noun'), ('의', 'Josa'), ('이름', 'Noun'), ('은', 'Josa'), ('뭐', 'Noun'), ('야', 'Josa'), ('?', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "print(okt.nouns(sent))\n",
    "print(okt.pos(sent))\n",
    "print(okt.pos(sent, stem=True)) # Automatically stems each words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### soynlp\n",
    "\n",
    "If we process words based on dictionaries as KoNLPy, **out of vocabulary** problem can occur as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['보', '보코', '코', '테러', '소말리', '전쟁']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "\n",
    "sent = \"보코하람 테러로 소말리아에서 전쟁이 있었어요.\"\n",
    "\n",
    "kkma = Kkma()\n",
    "\n",
    "print(kkma.nouns(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**soynlp** distinguishes the boundaries of words based on patterns that frequently occur in the learning data to solve o.o.v problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "from soynlp.word import WordExtractor\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "\n",
    "train_data = DoublespaceLineCorpus('/data.txt')\n",
    "\n",
    "noun_extractor = LRNounExtractor_v2()\n",
    "nouns = noun_extractor.train_extract(train_data) # returns nouns extracted from the data\n",
    "\n",
    "word_extractor = WordExtractor()\n",
    "words = word_extractor.train_extract(train_data) # returns words extracted from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 문서의 개수: 30091\n",
      "[Noun Extractor] use default predictors\n",
      "[Noun Extractor] num features: pos=3929, neg=2321, common=107\n",
      "[Noun Extractor] counting eojeols\n",
      "[EojeolCounter] n eojeol = 403896 from 30091 sents. mem=0.172 Gb                    \n",
      "[Noun Extractor] complete eojeol counter -> lr graph\n",
      "[Noun Extractor] has been trained. #eojeols=4434442, mem=0.918 Gb\n",
      "[Noun Extractor] batch prediction was completed for 119705 words\n",
      "[Noun Extractor] checked compounds. discovered 70639 compounds\n",
      "[Noun Extractor] postprocessing detaching_features : 109312 -> 92205\n",
      "[Noun Extractor] postprocessing ignore_features : 92205 -> 91999\n",
      "[Noun Extractor] postprocessing ignore_NJ : 91999 -> 90643\n",
      "[Noun Extractor] 90643 nouns (70639 compounds) with min frequency=1\n",
      "[Noun Extractor] flushing was done. mem=0.976 Gb                    \n",
      "[Noun Extractor] 76.63 % eojeols are covered\n",
      "90643\n",
      "['트와이스', '아이오아이', '1위']\n"
     ]
    }
   ],
   "source": [
    "from soynlp.utils import DoublespaceLineCorpus\n",
    "from soynlp.noun import LRNounExtractor_v2\n",
    "\n",
    "sent = '트와이스 아이오아이 좋아여 tt가 저번에 1위 했었죠?'\n",
    "\n",
    "# 학습에 사용할 데이터가 train_data에 저장되어 있습니다.\n",
    "corpus_path = 'articles.txt'\n",
    "train_data = DoublespaceLineCorpus(corpus_path)\n",
    "print(\"학습 문서의 개수: %d\" %(len(train_data)))\n",
    "\n",
    "# LRNounExtractor_v2 객체를 이용해 train_data에서 명사로 추정되는 단어를 nouns 변수에 저장하세요.\n",
    "noun_extractor = LRNounExtractor_v2()\n",
    "nouns = noun_extractor.train_extract(train_data)\n",
    "\n",
    "# 생성된 명사의 개수를 확인해봅니다.\n",
    "print(len(nouns))\n",
    "\n",
    "# 생성된 명사 목록을 사용해서 sent에 주어진 문장에서 명사를 sent_nouns 리스트에 저장하세요.\n",
    "sent_nouns = []\n",
    "for word in sent.split():\n",
    "    if word in nouns:\n",
    "        sent_nouns.append(word)\n",
    "\n",
    "print(sent_nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity between sentences\n",
    "\n",
    "#### Jaccard Similarity\n",
    "\n",
    "$$\\text{Jaccard Similarity between } Sent_1 \\text{ and } Sent_2 = \\frac{\\text{number of common words in two }Sents}{\\text{number of all the words in two }Sents}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sent_1 = \"오늘 중부지방을 중심으로 소나기가 예상됩니다\"\n",
    "sent_2 = \"오늘 전국이 맑은 날씨가 예상됩니다\"\n",
    "\n",
    "def tokenize(sent):\n",
    "    words_sent = set()\n",
    "    for word in sent.split():\n",
    "        words_sent.add(word)\n",
    "    return words_sent\n",
    "\n",
    "# Implementing function that returns jaccard similarity\n",
    "def cal_jaccard_sim(sent1, sent2):\n",
    "\n",
    "    words_sent1 = tokenize(sent1)\n",
    "    words_sent2 = tokenize(sent2)\n",
    "    \n",
    "    intersection = len(words_sent1 & words_sent2)\n",
    "    \n",
    "    union = len(words_sent1 | words_sent2)\n",
    "\n",
    "    return float(intersection / union)\n",
    "\n",
    "# cal_jaccard_sim() 함수 실행 결과를 확인합니다.\n",
    "print(cal_jaccard_sim(sent_1, sent_2))\n",
    "\n",
    "# Using jaccard_distance function from nltk to implement function that returns jaccard similarity. \n",
    "set1 = tokenize(sent_1)\n",
    "set2 = tokenize(sent_2)\n",
    "nltk_jaccard_sim = 1 - nltk.jaccard_distance(set1, set2)\n",
    "\n",
    "# This function returns the same value as cal_jaccard_sim too.\n",
    "print(nltk_jaccard_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Similarity\n",
    "\n",
    "**Cosine Similarity** is calculated based on an **angle between sentence vectors**.\n",
    "\n",
    "$$\\text{Cosine Similarity between } A \\text{ and } B = \\frac{A * B}{||A|| ||B||}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7137224896052109\n",
      "0.7137224896052109\n",
      "[[1.         0.71372249 0.4876509 ]\n",
      " [0.71372249 1.         0.2801926 ]\n",
      " [0.4876509  0.2801926  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import sqrt, dot\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import pairwise\n",
    "\n",
    "sent_1 = [0.3, 0.2, 0.2133, 0.3891, 0.8852, 0.586, 1.244, 0.777, 0.882]\n",
    "sent_2 = [0.03, 0.223, 0.1, 0.4, 2.931, 0.122, 0.5934, 0.8472, 0.54]\n",
    "sent_3 = [0.13, 0.83, 0.827, 0.92, 0.1, 0.32, 0.28, 0.34, 0]\n",
    "\n",
    "# Implement function that returns cosine similarity\n",
    "def cal_cosine_sim(v1, v2):\n",
    "    return dot(v1, v2) / (sqrt(dot(v1, v1)) * sqrt(dot(v2, v2)))\n",
    "    \n",
    "print(cal_cosine_sim(sent_1, sent_2))\n",
    "\n",
    "# Use distance.cosine() from scipy to calculate cosine similarity.\n",
    "scipy_cosine_sim = 1 - distance.cosine(sent_1, sent_2)\n",
    "\n",
    "print(scipy_cosine_sim)\n",
    "\n",
    "# Use pairwise.cosine_similarity() from scikit-learn to calculate cosine similarity.\n",
    "all_sent = [sent_1] + [sent_2] + [sent_3]\n",
    "scikit_learn_cosine_sim  = pairwise.cosine_similarity(all_sent)\n",
    "\n",
    "print(scikit_learn_cosine_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
