{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "## 5️⃣ Document Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Similarity\n",
    "\n",
    "Documents can be expressed in **words**, which are the most basic units of documents. Too calculate **similarity between documents**, we will use **cosine similarity** of **document vectors** created based on words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "\n",
    "**Bag of Words** is one of the ways to create **document vector**, which creates the vector using **frequency of each words in the document**.\n",
    "\n",
    "The **dimension** of Bag of words document vectors is the same as **the number of all words in the document**.\n",
    "\n",
    "The bag of words document vector has a problem in that it **cannot preserve the meaning of the compound word** by treating the compound word as an independent word.\n",
    " (i.e. 'log off' -> 'log' and 'off')\n",
    "\n",
    "#### Bag of N-grams\n",
    "\n",
    "**N-gram** analyzes text based on N consecutive words, so that it preserve the meaning of the compound word.\n",
    "\n",
    "- N = 1 (unigram) : [\"포근한\", \"봄\", \"날씨가\", \"이어질\", \"것으로\", \"전망되며\", ...]\n",
    "- N = 2 (bigram) : [\"포근한 봄\", \"날씨가 이어질\", \"것으로 전망되며\", ...]\n",
    "- N = 3 (trigram) : [\"포근한 봄 날씨가\", \"이어질 것으로 전망되며\", ...]\n",
    "\n",
    "**Bag of N-grams** represent document vectors based on frequency of combined several N-grams having various N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(454, 12640)\n",
      "['aal', 'aba', 'abandon', 'abandoned', 'abbot', 'abducted', 'abets', 'abilities', 'ability', 'abilitytalent']\n",
      "2129\n",
      "  (0, 9686)\t4\n",
      "  (0, 5525)\t4\n",
      "  (0, 6010)\t4\n",
      "  (0, 1761)\t1\n",
      "  (0, 2129)\t1\n",
      "  (0, 9081)\t1\n",
      "  (0, 948)\t2\n",
      "  (0, 11184)\t8\n",
      "  (0, 9829)\t1\n",
      "  (0, 11321)\t1\n",
      "  (0, 870)\t2\n",
      "  (0, 10446)\t1\n",
      "  (0, 8064)\t1\n",
      "  (0, 8847)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 9910)\t2\n",
      "  (0, 6444)\t1\n",
      "  (0, 10842)\t1\n",
      "  (0, 3245)\t2\n",
      "  (0, 12582)\t1\n",
      "  (0, 5689)\t2\n",
      "  (0, 11070)\t1\n",
      "  (0, 8837)\t1\n",
      "  (0, 6343)\t1\n",
      "  (0, 6826)\t2\n",
      "  :\t:\n",
      "  (0, 9400)\t1\n",
      "  (0, 11527)\t1\n",
      "  (0, 1635)\t1\n",
      "  (0, 3144)\t1\n",
      "  (0, 5633)\t1\n",
      "  (0, 9213)\t1\n",
      "  (0, 2012)\t1\n",
      "  (0, 6479)\t1\n",
      "  (0, 5126)\t1\n",
      "  (0, 9794)\t1\n",
      "  (0, 7803)\t1\n",
      "  (0, 12607)\t1\n",
      "  (0, 3420)\t1\n",
      "  (0, 3978)\t1\n",
      "  (0, 6750)\t1\n",
      "  (0, 196)\t1\n",
      "  (0, 7185)\t1\n",
      "  (0, 257)\t1\n",
      "  (0, 11237)\t1\n",
      "  (0, 4134)\t1\n",
      "  (0, 4212)\t1\n",
      "  (0, 4996)\t1\n",
      "  (0, 8519)\t1\n",
      "  (0, 6046)\t1\n",
      "  (0, 6028)\t1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "regex = re.compile('[^a-z ]')\n",
    "\n",
    "with open(\"test.txt\", 'r') as f:\n",
    "    documents = []\n",
    "    for line in f:\n",
    "        documents.append(regex.sub(\"\",line.rstrip()))\n",
    "        \n",
    "# Create Bag of words document vector using CountVectorizer() object, then save it to X.  \n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(documents)\n",
    "\n",
    "# Print X's dimension\n",
    "dim = X.shape\n",
    "print(dim)\n",
    "\n",
    "# Print words that first 10 columns of X express using get_feature_names().\n",
    "words_feature = cv.get_feature_names()[:10]\n",
    "print(words_feature)\n",
    "\n",
    "# Find the index of column meaning \"comedy\".\n",
    "idx = cv.get_feature_names().index(\"comedy\")\n",
    "print(idx)\n",
    "\n",
    "# Save first document's Bag of words vector to vec1, then print it.\n",
    "vec1 = X[0]\n",
    "print(vec1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "**TF-IDF**(term frequency - inverse document frequency) reflects that relatively more frequently occurring words in a document have a more important meaning for the document.\n",
    "\n",
    "$$\\text{TF-IDF of } word_1  \\text{ in } doc_1 = \\frac{\\text{frequency of }word_1\\text{ in }doc_1}{\\text{frequency of every words in }doc_1} * log(\\frac{\\text{number of every docs in data}}{\\text{number of docs that contains }word_1\\text{ in data}}) $$\n",
    "\n",
    "The **TF-IDF-based bag of words document vector** lowers the importance of words that appear frequently in all documents and increases the importance of words that occur frequently only in specific documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(454, 12136)\n",
      "  (0, 5679)\t0.058640619958889736\n",
      "  (0, 8003)\t0.10821800789540346\n",
      "  (0, 11827)\t0.03351360629176965\n",
      "  (0, 3976)\t0.10821800789540346\n",
      "  (0, 3885)\t0.056559253324120214\n",
      "  (0, 10825)\t0.040897765011371726\n",
      "  (0, 228)\t0.07670128660443204\n",
      "  (0, 173)\t0.08289290212342751\n",
      "  (0, 6546)\t0.043212451333837304\n",
      "  (0, 3731)\t0.06944792122696908\n",
      "  (0, 11793)\t0.08712444266241767\n",
      "  (0, 12103)\t0.05368632319734369\n",
      "  (0, 7470)\t0.025687260438575044\n",
      "  (0, 9189)\t0.10821800789540346\n",
      "  (0, 4994)\t0.04450120519256354\n",
      "  (0, 5326)\t0.05241495461089306\n",
      "  (0, 5538)\t0.09654704887371249\n",
      "  (0, 6240)\t0.05908965016142883\n",
      "  (0, 1896)\t0.06002531501567428\n",
      "  (0, 8681)\t0.10139093452026096\n",
      "  (0, 5344)\t0.08289290212342751\n",
      "  (0, 3162)\t0.05211156559734475\n",
      "  (0, 1438)\t0.09278983927035103\n",
      "  (0, 11136)\t0.07804901647687901\n",
      "  (0, 8858)\t0.09654704887371249\n",
      "  :\t:\n",
      "  (0, 8333)\t0.09654704887371249\n",
      "  (0, 10655)\t0.10821800789540346\n",
      "  (0, 5394)\t0.036855273761029705\n",
      "  (0, 12074)\t0.045486142952180335\n",
      "  (0, 7080)\t0.06950749415600704\n",
      "  (0, 10653)\t0.43287203158161386\n",
      "  (0, 10410)\t0.04631780012339499\n",
      "  (0, 6199)\t0.045486142952180335\n",
      "  (0, 9332)\t0.14858361374703516\n",
      "  (0, 22)\t0.030847058442278544\n",
      "  (0, 8343)\t0.09654704887371249\n",
      "  (0, 7576)\t0.03767413181742104\n",
      "  (0, 9948)\t0.03226580332806431\n",
      "  (0, 632)\t0.04595176416002607\n",
      "  (0, 10915)\t0.03552172020508319\n",
      "  (0, 9232)\t0.05368632319734369\n",
      "  (0, 10769)\t0.1528768111964441\n",
      "  (0, 683)\t0.05694325708073058\n",
      "  (0, 8584)\t0.08971997549857\n",
      "  (0, 5685)\t0.038437344953303164\n",
      "  (0, 2038)\t0.05617216421425451\n",
      "  (0, 1605)\t0.08712444266241767\n",
      "  (0, 5672)\t0.07371054752205941\n",
      "  (0, 5036)\t0.2575794789063761\n",
      "  (0, 1356)\t0.43287203158161386\n",
      "(454, 74358)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "regex = re.compile('[^a-z ]')\n",
    "\n",
    "with open(\"test.txt\", 'r') as f:\n",
    "    documents = []\n",
    "    for line in f:\n",
    "        lowered_sent = line.rstrip().lower()\n",
    "        filtered_sent = regex.sub('', lowered_sent)\n",
    "        documents.append(filtered_sent)\n",
    "\n",
    "# Create TF-IDF Bag of words document vector using TfidVectorizer() object, then save it to X.  \n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(documents)\n",
    "\n",
    "# Print X's dimension\n",
    "dim1 = X.shape\n",
    "print(dim1)\n",
    "\n",
    "# Save fisrt document's TF-IDF Bag of words vector to vec1.\n",
    "vec1 = X[0]\n",
    "print(vec1)\n",
    "\n",
    "# Create TF-IDF Bag of N-grams document vector using TfidfVectorizer() object.\n",
    "unibigram_tfidf = TfidfVectorizer(ngram_range = (1, 2)) # This vectorizer uses unigram and bigram.\n",
    "unibigram_X = unibigram_tfidf.fit_transform(documents)\n",
    "\n",
    "\n",
    "# Print unibigram_X's dimension\n",
    "dim2 = unibigram_X.shape\n",
    "# 문서 벡터의 차원을 확인합니다.\n",
    "print(dim2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
