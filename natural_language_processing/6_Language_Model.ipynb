{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "## 6️⃣ Language Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model\n",
    "\n",
    "**Language model** is a model that calculates the probability that a given sentence occurs in the text data.\n",
    "\n",
    "The probability that a given sentence occurs in the text data can be calculated by products of the conditional probabilty that each words occurs.\n",
    "\n",
    "$$P(sentence1) = P(word_1) * P(word_2 | word_1) * P(word_3 | word_1, word_2) * P(word_4 | word_1, word_2, word_3) * P(word_5 | word_1, word_2, word_3, word_4) * ...$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram Language Model\n",
    "\n",
    "Unlike Statistical Language Model, N-gram language model predicts next word **only based on *N-1* words**.\n",
    "\n",
    "$$P(sentence1) \\approx P(word_3 | word_1, word_2) * P(word_4 | word_2, word_3) * P(word_5 | word_3, word_4) * ...$$\n",
    "\n",
    "Each N-gram-based conditional probability is calculated using **each n-gram's frequency in the data**.\n",
    "\n",
    "$$P(word_3 | word_1, word_2) = \\frac{\\text{frequency of }word_1, word_2, word_3\\text{ in the entire data}}{\\text{frequency of }word_1, word_2\\text{ in the entire data}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "data = ['this is a dog', 'this is a cat', 'this is my horse','my name is elice', 'my name is hank']\n",
    "\n",
    "def count_unigram(docs):\n",
    "    unigram_counter = dict()\n",
    "    # Save frequency of every unigram in data to unigram_counter.\n",
    "    for line in docs:\n",
    "        for word in line.split():\n",
    "            if word not in unigram_counter:\n",
    "                unigram_counter[word] = 1\n",
    "            else:\n",
    "                unigram_counter[word] += 1\n",
    "    return unigram_counter\n",
    "\n",
    "def count_bigram(docs):\n",
    "    bigram_counter = dict()\n",
    "    # Save frequency of every bigram in data to bigram_counter.\n",
    "    for line in docs:\n",
    "        previous_word = line.split()[0]\n",
    "        for word in line.split()[1:] :\n",
    "            bigram = (previous_word, word)\n",
    "            if bigram not in bigram_counter:\n",
    "                bigram_counter[bigram] = 1\n",
    "            else:\n",
    "                bigram_counter[bigram] += 1\n",
    "            previous_word = word\n",
    "    \n",
    "    return bigram_counter\n",
    "\n",
    "def cal_prob(sent, unigram_counter, bigram_counter):\n",
    "    words = sent.split()\n",
    "    result = 1.0\n",
    "    previous_word = words[0]\n",
    "    \n",
    "    for word in words[1:]:\n",
    "        top = bigram_counter[(previous_word, word)]\n",
    "        bottom = unigram_counter[previous_word]\n",
    "        result *= float(top/bottom)\n",
    "        previous_word = word\n",
    "    \n",
    "    return result\n",
    "\n",
    "unigram_counter = count_unigram(data)\n",
    "bigram_counter = count_bigram(data)\n",
    "\n",
    "print(cal_prob(\"this is elice\", unigram_counter, bigram_counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Language Model\n",
    "\n",
    "We can use **RNN** to create language model.\n",
    "\n",
    "When each word in a sentence is given, the RNN model can be trained using a problem of predicting the next word of each words.\n",
    "\n",
    "Also, it is possible to process and generate words that did not exist in the learning data through character-by-character data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
